\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\bibliographystyle{besjournals}

\begin{document}

\title{Closing the gap between statistical and scientific workflows for improved forecasts in ecology } 
\date{\today}
\author{Victor Van der Meersch, J. Regetz, T. J. Davies$^*$ \& EM Wolkovich}
\maketitle
$^*$ Says he is happy to help and give friendly review, but not sure he will reach level of co-author. 

{\bf Deadline:} 1 May 2025 (was 1 April 2025)

\emph{For:} Scientific and Statistical Workflow theme issue for \emph{Phil Trans A} as an \emph{Opinion}
% They mention a tex template here: https://royalsocietypublishing.org/rsta/for-authors#question4
% Word length: I should check what they emailed but they say no more than 13 printed pages with 650 per page (whoa, we should be well below that! I am thinking 3-5K would be good)

\begin{abstract}
Increasing biodiversity loss and climate change have led to greater demands for useful ecological models and forecasts. Relevant datasets to meet these demands have also increased in size and complexity, including in their geographical, temporal and phylogenetic scales. While new research often suggests that accounting for these complexities variously increases, removes or otherwise alters major trends, I argue that the fundamental approach to model fitting in ecology makes it impossible to evaluate and compare models. These problems stem in part from continuing gaps between statistical workflows -- where the data processing and model development are often addressed separately from the ecological question and aim -- and scientific workflows, where all steps are integrated. Yet, as ecologists become increasingly computational, and new tools make it easier to share data, the opportunity to close this gap has never been greater. I outline how increased data simulation at multiple steps in the scientific workflow could revolutionize our understanding of ecological systems, yielding new insights. Combining these changes with more open model and data sharing -- and developing new efforts to race the same data -- could be transformative for ecological forecasting. 
\end{abstract}

{\noindent \bf Goal:} Increase awareness of how we can merge statistical and scientific workflows in ecology (especially forecasting) and what we would get out of it.
\vspace*{0.5cm}

\noindent \textbf{Introduction}
% Still need some work...

% Ecology has become super challenged to predict stuff for decision making
Nature is increasingly threatened by multiple drivers of change, with a largely dominant influence of human activities \citep{Diaz2019}. This ongoing biodiversity crisis is expected to increase in the next decades because of climate change, and will continue to alter ecosystem services and human well-being \citep{IPBES2019}. To support implementation of sustainable policies among the socioeconomic and environmental dimensions, it is critical to understand trends to date and be able to forecast future dynamics. 
% $\rightarrow$ Example of stuff to predict: population size and geographic distribution (terrestrial, freshwater, marine), extinction risk,...

% General way to do this so far... (bifurcated?)
Estimation of global biodiversity indicators and current trends depends on large-scale and long-term datasets---across terrestrial, freshwater, and marine ecosystems \citep[e.g.][]{Dornelas2018}. These data, gathered opportunistically and from multiple sources, are often unbalanced and have geographic, temporal and taxonomic biases. Addressing these biases requires the use of appropriate statistical inference.
Forecasting future changes---under different plausible scenarios---generally relies on either correlative models or process-based models \citep{IPBES2019}. The latter, which focus on a mechanistic representation of ecosystem functioning, are often promoted as the most realistic approach \citep{Urban2016, Pilowsky2022}. % ... lacks something

% Gap, problem: None of this is going well
The urgent need to answer policy-relevant questions has favored the proliferation of diverse methods developed by different researchers, lacking an overall coherence. Though there is no doubt nature is declining globally, significant uncertainty remains. There is no consensus on current species trends, with ongoing debates driven by widely varying reports that sometimes show conflicting trend directions \citep{Dornelas2014, Leung2020, Buschke2021, Johnson2024}. Future projections also diverge considerably, due to a high model uncertainty at the ecological level. Predictive modeling is increasingly relying on overly complex models (with a huge number of parameters), making it less adequate to generate new scientific insights \citep{Franklin2020}.

% Current workflows in ecology are not up to the task
% Here we introduce a universal workflow to adress this...
Current controversies and focus on methodological aspects stem from the lack of coherence between current workflows in ecology \citep{Loreau2022, Talis2023, Johnson2024}. Each new model development is added as a separate layer, disconnected from the original research aim, the data stream and the previous scientific insights. Workflows should fully integrate all the steps required to build a model from an ecological question, evaluate its limitations and degeneracies, before estimating its parameters and making projections. Here, we introduce an universal workflow that proposes to iteratively build upon all these steps, harmonizing both trend estimation and forecasting, with the aim of refocusing the debate on ecological questions and increasing the speed of scientific progress.

\vspace{0.5cm}
\noindent \textbf{Scientific method and workflows}
%  too much blah blah blah?

% General scientific method we all learn
Quantitative science rely on a model-based framework to confront hypothesis and data, making some approximations \citep{}. The general scientific method stresses out that the research question should guide both the design of the experiment and the corresponding model building. The experiment should then be conducted---according to this specific design. Finally, the resulting experimental data should be used to inform our model and differentiate between hypotheses---hopefully answering our initial question. However, this is often an idealized view.
% and does not reflect the complexity of ecology \citep{}. I already say it below

% Divergences from this are common 
Divergences from this scientific method are common. One explanation is the lack of rigor, leading to questionable practices, such as retrospectively crafting hypothesis to explain the results of a model rather than testing a clear pre-defined question (data-driven analysis, Fig. 1). But the reality of ecological research is also a major driver of the divergences from the ideal scientific method. Many important questions often cannot be addressed by conducting experiments and replications, and we must often rely on existing datasets to have a large-scale and long-term perspective \citep{Hilborn1997}. % I feel something is missing
However, this does not explain the persistent flaws and lack of overall coherence in ecological modeling. Trend estimation still often relies on a one-way 'inference',  % need a better word? to stress out that there is no feedback, eah paper applied a slightly different model 
and most of the time is spent fitting the model to empirical data (Fig. 1). For forecasting, researchers focus on making predictions with complex models, but the steps of model building and parameterization are not very transparent and not clearly delineated (Fig. 1). The different parts of the model are often calibrated separately rather than as a whole, and some parameter values are just fixed based on experiments and expert knowledge.

% There is thus room for improvment, with a workflow that addresses theses flaws while taking account the reality of working with ecological data.
% Moving along the data-model space, step-by-step
% \item stressing the need to think about model before study design
% \item 
To address these flaws while accounting for the realities of working with ecological data, a comprehensive workflow is needed to move along the data-model space, following a coherent sequence of steps (Fig 1). In particular, more efforts should be placed on model evaluation before incorporating any real data. This would force the modeler to acknowledge that some parameters might be non-identifiable and to reconsider the model structure. Similarly, it is essential to assess whether model predictions---once parameters are informed by data---are consistent with observations. The strength of such workflow lies in its flexibility, making it applicable to a wide range of modeling approaches, from simple trend analyses to more complex process-based models. At each step, the modeler need to critically examine its understanding of ecological processes, questioning previous assumptions, and explicitly acknowledge sources of uncertainty. This approach has the potential to enhance model interpretability and allow for a more transparent evaluation of model strengths and limitations. It also replace parameters at the core of the modeling process, as fundamental components that shape both inference and forecasting. % I think we would need to explain more this idea about parameters... of keep it for later

% More details on the workflow:  walk through the different steps
% the paragraph is a bit loooong
The first step of the workflow is to define an explicit research question and formulate some hypotheses (Fig 2). This involves making assumptions about the most influential drivers, within the specific context of our study. This step help us to think about the mechanisms that could generate the data we observe, including the observational error. Naturally, this leads to the development of a model---an ensemble of mathematical equations that encapsulates our knowledge and designed to answer our research question. The general idea is to start with a relatively simple model, that we could refine later. At this stage, prioritizing biologically meaningful parameters is crucial, as it allows us to have a sense of plausible parameter values. The next step is to simulate data that reflect our assumptions about the data structure and biases, by fixing parameter values to some values (which is straightforward if the parameters are interpretable). We then fit our model to this simulated dataset and evaluate its ability to recover the prescribed values. 
% Some issues might arise at this step
Structural degeneracies in the model might become apparent, revealing that some parameters are highly non-identifiable. This is the first feedback loop: if the model is not able to recover known parameter values, we must reconsider its structure. This way, we ensure the model works well on simulated data before incorporating real observations. We can then fit the model to real data, to obtain parameter estimates constrained by observations. Here, difficulty in fitting the model might indicate an inherent need for more data to address our initial question. This could lead us to either simplify our research question or---ideally---launch new data collection efforts. The next step is a new data simulation step, this time using our fitted model to generate predictions. This retrodictive check allows model output to be compared to observations. Discrepancies may indicate a missing key driver---perhaps an expected outcome if we known our initial model was too simplistic. This is a second feedback loop. We can refine the model to integrate the missing process and restart the workflow. Insights from the retrodictive check can also lead us to introduce additional complexity when simulating fake data, such as phylogenetic structure or observational biases (e.g. unbalanced data). This iterative evaluation of the model moves beyond a simple reliance on goodness-of-fit metrics. At each iteration, we are able to evaluate the model behavior, both with simulated and real data, taking into account our expert knowledge of the ecological processes. Within such a workflow, forecasting emerges as a natural outcome: rather than being a final goal, it only involves jointly modeling new circumstances along with the original data.


\vspace{0.5cm}
\noindent \textbf{The workflow in practice}
% How to address current issues (two case studies)

% Trends - outline current problem
%  Evidence of declining populations of vertebrate species in the 20th century gave rise a new subfield within ecology---conservation biology---now an entire discipline of its own that drives much of the policy-relevant science in ecology. 
Evidence of declining populations of vertebrate species in the latter half of the 20th century, alongside increasing ecosystem health concerns, led to growing public concern about protecting and maintaining the environment, challenging ecology to help predict and prevent further losses \citep{soule1999conserving,soule1991conservation}. The idea that important taxa were declining was clear from data from certain species and their populations, such as elephants and rhinos \citep{soule1979benign,leader1990illegal}. Such trends drove a number of new subfields within ecology---some of which are now complete disciplines within themselves \citep[such as conservation biology,][]{soule1985conservation}---focused on these problems, and potential ecological solutions to them CITES. Yet, as the magnitude and number of threats to these and other species have increased, with rates of habitat loss, overharvesting, pollution only increase, and anthropogenic climate change now clearly driving species loss \citep{waller2017bramble}, so has the data and its complexity. 

% \cite{loh2005living} is first LPI paper as best I know
% \cite{collen2009monitoring,ledger2023past,leung2022reply,puurtinen2022living,toszogyova2024mathematical} to review!
Ecologists have now amassed data on populations and species over the globe, they have also engaged in an increasing number of debates on regional and global trends, with arguments over the magnitude and even direction \citep{Dornelas2014,Leung2020,terry2022no,muller2024weather}. The Living Planet Index (LPI), which aims to include long-term data on vertebrate populations of species across the globe is emblematic of these debates. With updated data released semi-annually (??) alongside new estimates of decline, a growing number of high-profile papers have challenged how strong the evidence is for population decline \citep{Dornelas2014,gonzalez2016estimating,wagner2021insect,muller2024weather}, with each paper taking a slightly different analytical approach. For example, \citet{Leung2020} published a mixture model that suggested most populations were not significantly declining, followed by other alternative modeling approaches \citep{Buschke2021,puurtinen2022living} including a recent one suggesting a basic analysis of the dataset should always include three sources of autocorrelation, finding trends that encompassed most previous results \citep{Johnson2024}. 

Apparently conflicting results have made it harder for policy-makers to advance initiatives aimed at slowing declines, and has led to a debates within ecology about whether such analyses undermine public confidence in science  \citep{gonzalez2016estimating}. While shifting estimates are part of the process of science---refining our approaches and thus estimates over time---we argue much of the work underlying these debates stems from a poor workflow. In the current workflow for estimating trends over time a new model with a new estimate often leads to a paper (see Fig.) because ecologists spend far too little time interrogating their models with simulated data, or their model performance fit to empirical data. Functionally, research on the LPI has somewhat reverse-engineered the recommended workflow: after a series of papers debating different estimates from different models, more recent papers have focused on simulated data to highlight uncertainty given the model and data togethers \citep[though I don't think they link their simulations to the model they use that well,][]{dove2023quantifying,toszogyova2024mathematical}, but this should have been part of the process for the very first papers.
% While this debate captures part of the process of science---refining our approaches and thus estimates over time---it also makes it hard for policy-makers to do (?? something) and has led to a debates within ecology about whether such analyses undermine public confidence in science  (CITES Biodiversity debate). 
% \citet{Buschke2021} who focused on adding random population variation

We argue than an improved workflow that required retrodictive checks and data simulation would lead to larger model advances and a greater recognition of uncertainty---thus highlighting likely consistency in estimates across models---that could better aid policy.  Using the workflow would make what now appear as major discrepancies more obviously shifts in point estimates that are generally all in the same uncertainty space \citep{Johnson2024}---and it would challenge modelers to show major predictive advances, which is not currently part of the process. Explanatory power in most models of observational data is usually very low \citep{low2014rising,moller2002much} and thus tests of models' predictions rarely expected. But the workflow highlights that predictions from the model---what we call retrodictive checks (or whatever we call them)---are part of the process of science, and critical to testing for what may be missing in a model. We expect retrodictive checks on most published trend analyses would highlight major missing components in these models (expand here?? ADD example?), and drive changes both in the models themselves and in the simulated data to check the models. While ecologists have started to use simulated data more to understand potential limitations of their models and data combined, this is still extremely rare, and efforts to date often treat simulations as separate from the statistical model \citep{Buschke2021,dove2023quantifying}, short-circuiting their full utility
% Mention how low R2 are in ecology?
% Simulating data to verify and test models in trend analyses is currently rarely reported, which we believe makes it easier for ecologists to fit poor models and in turn debate them.. 
% Mention elephants rebounding...

% Trends - improvements!
Applying the workflow to current trend estimates could importantly highlight the best way to improve data collection for more reliable estimates. Returning to the example of a global estimate of trends in vertebrate populations of species over time and applying our proposed workflow would mean more efforts to define the goal and question---is it a simple global estimate? Or a need to also find which species are declining most, including those that may have poor or no data? From there a generative model using simulated data for testing could incorporate many aspects of the populations, and data, that are often only included in `null' or 'synthetic data generation' currently \citep{Buschke2021,mcrae2025utility} but could be built into the models fit to the empirical data. Eventually fitting the empirical data and performing retrodictive checks would likely highlight major missing components of the generative model. For example, certain populations are recovering for very specific reasons (e.g., elephants in regions where the ivory trade drove declines in the past) that perhaps should be modeled. From this model, what data are most critically needed to address the updated aims would become clearer and could drive new data collection \citep{toszogyova2024mathematical}. 

% Forecasting - outline current problem
Ecological forecasting is a broad field with a diverse range of methods. Process-based modeling is often considered the gold standard in ecology \citep{Urban2016, Pilowsky2022} and beyond. Process-based models are built on explicit mathematical equations to describe (supposedly causal) relationships between environmental drivers and ecological responses. They also often incorporate empirical relationships, particularly when knowledge is incomplete or when some processes are intentionally omitted. Processes are often represented at different nested spatiotemporal scales, depending on the underlying assumptions. Model development is the central step, typically requiring several years, yet it often remains opaque from an external perspective. The step of designing the model---translating knowledge and hypotheses into mathematical equations and parameters---is often blurred with the step of model calibration (or tuning), where parameter values are inferred. Models are often treated as an accumulation of multiple submodels, each governing one or several ecological processes. Rather than being fitted as a whole, submodels are calibrated separately against specific subsets of data, and some parameters are simply prescribed (i.e. fix to a value found in the literature) or tuned to reproduce some observations or theory. The way models are currently calibrated is not a coincidence, % is coincidence the right word? 
but rather an inappropriate way to accommodate their complexity, where many parameters compensate for one another.

These limitations are central to current debates. In climate modeling, increasing model complexity has not necessarily led to reduced uncertainty. For instance, the uncertainty range on the effect of increasing CO2 concentration on temperature have remained largely unchanged \citep{Zelinka2020}. This has driven calls for more rigorous and transparent calibration processes \citep{balaji2022general}. Similar concerns arise in ecology, where strong discrepancies exist between model projections \citep{Cheaib2012}. Some researchers now advocate for the simplification of models, to avoid over-parametrization when the data provide little information to constrain some parameters \citep{Wang2017, Harrison2021}. If a model becomes too complex, understanding the sources of uncertainty and how they propagate through the model may become nearly impossible.
Each additional process and parameter can increase overall uncertainty to the point where predictions lose their usefulness for decision makers \citep{Saltelli2020}.

% Forecasting - potential improvments (do I say too much ideas)
Applying the workflow to process-based models is a key for opening the black box. It would serve as a guide through the successive steps of model development. In particular, incorporating data simulation would introduce a crucial step between model building and data fitting, ensuring a clear delineation between the two and exposing strong degeneracies in the model design. This approach would force researchers to begin with a simpler version of the model, providing a clear pathway to support---or reject---the additional complexity and new parameters along the iterative development of the model. 
% => quantifying parameter uncertainty
% incorporate additional observations and their respective uncertainties
Model calibration would no longer be just a hidden aspect of model building but a step as crucial as forecasting to gain new ecological insights. 
% It would help properly take into account for any issues regarding non-identifiability.
The workflow could also refocus attention on the research question, defining a clear and limited context in which the model should apply.
Process-based model would once again be a way to answer a research question---whereas today, model simulations have increasingly become a subject of study on their own. An universal workflow provides an opportunity to merge statistical and process-based approaches, integrating mechanistic knowledge and leveraging robust statistical approaches. 

% Step back
%\item we need more data, and better question (relate this to both previous study cases)
%\item where can we best reduce uncertainties through new scientific insights?
%\item machine learning! If we change nothing, what's the point of not doing ML? ML $>$ process-based without question, and ML $>$ trends without mechanisms! Where theory is lacking, or where we are far from mechanistic understanding, you might as well do ML!
%\item (ML has a way to collect and interpret large datasets...)
Across the different fields of ecology---for both parameter estimation and forecasting---a systematic application of a coherent workflow holds the promise to highlight the opportunities to best reduce uncertainties through new scientific insights, toward the most critical steps. This will help refocus the debate on designing new hypothesis, formulating new questions---and guiding efforts to collect new data. 
In a world where machine learning is rapidly advancing, there is no point of sticking to traditional methods if no changes are made. Machine learning may surpass process-based models if the latter lack a robust estimation of their parameters and fall in a complexity trap, at the cost of their interpretability. Similarly, trend analysis, when the focus is on methodological controversies (due to the lack of an iterative workflow) rather than on a robust mechanistic foundation, offers no clear advantage over machine learning.
% If we do not take care to apply this workflow, machine learning might be the more pragmatic approach...

% Misc possibly useful citations -- the GCM people seem to talk a lot about their issues lately on tuning etc. Maybe worth citing?
% \cite{balaji2022general}mAre general circulation models obsolete? -- covers a lot and good citations within I think
% \cite{simpson2025confronting} this is the 'Confronting Earth System Model trends with observations' which I have not fully read but Table 1 is interesting to me and something maybe the PBM and NTIF (near-term iterative) people should be doing if they are not doing?
% \cite{rounce2020quantifying} 'Quantifying parameter uncertainty in a large-scale glacier evolution model using Bayesian inference: application to High Mountain Asia' talks about identifablity problems and uses Bayesian, again I have not fully read, but may be interesting example?

\vspace{0.5cm}
\noindent \textbf{Wrap up: how to make it happen?}

The misuse of models and misinterpretation of their outcomes (including by the authors) have multiple origins. One cause, well known, is the pressure to publish academic work, which can lower research standards. This challenge extends beyond the scope of this paper, yet we believe that applying an intelligible workflow can promote better research ethics.
This a growing concern, leading to increase in reproducibility and data sharing practices.
However, model development in ecology is still rarely transparent, which limits model understandability and prevents peers from properly identifying potential issues. A significant portion of scientific debate thus becomes lost in methodological considerations rather than advancing our ecological understanding.

% better training! on BOTH estimation AND prediction
% learn how to simulate data will allow students to understad what is the link between the maths (eg a distribution) and the ecological processes that could be a l'oeuvre to generate the observations, make better links bewteen disciplines
% \item estimation: being aware of what is a parameter, and mention uncertainty propagation
% \item forecasting: should be a natural outcome, not a finality

% \item ML (short-time forecast), benchmarking models are probably useful but should not be the core of our scientific practice, not the spirit!\\
% $\rightarrow$ moving ecology in the right direction!



%\item What are current workflows and where are they limiting us?
%\begin{enumerate}
%\item For trends ...
%\begin{enumerate}
%\item easy to find different trends through small model tweaks to analyses and/or different data
%\item For example, right now many different papers report different biodiversity trends (LPI example?)
%\item New workflow would make ecologists understand uncertainty in their model data/combo (and perhaps not see/publish results as so divergent?)
%\end{enumerate}
%\item For forecasting ... (somehow jump to our focus on process-based models PBMs here? Something like, forecasting is big and there are diverse methods! Near-term iterative, correlative niche models, but PBMs are often considered the gold standard ... mention machine learning?)
%\begin{enumerate}
%\item as many models as researchers working on process-based models 
%\item + accumulation of successive layers in the development of models = significant challenge to scientific transparency, reproducibility, interpretability\\
%models often draw inspiration from each other, which is good (way to do science), but not always explicit... (some issues: arbitrarily established parameter value in one model then transmitted to multiple models)
%\item focus of researchers: always integrate new mechanisms, new parameters, to increase "realism"...  they intuitively "feel" what kinds of adjustments is needed... but opaque from an external perspective ("black box" of model building and calibration)
%\item models rarely fitted as a whole, dozens of parameters without explicitly quantifying parameter uncertainty, and often neglect to propagate this uncertainty
%\item simulations of models themselves became a subject of study to disentangle all the processes modelled and understand model sensitivity 
%\end{enumerate}
%\end{enumerate}
%\item Better workflows to the rescue! 
%\begin{enumerate}
%\item General overview of new workflows
%\begin{enumerate}
%\item Step 0: Research Qs and hypotheses (with a mechanism) lets you ...
%\item Step 1: Build a model!
%\item Step 2: Simulate data (and priors or something like priors that forces you to put numbers on stuff)
%\item Step 3: Design experiments/data collection (maybe you go back to Step 1 here)
%\item Step 4: Simulate data from actual design/collection
%\item Step 5: Fit the model to empirical data
%\item Step 6: Retrodictive checks (feed back to 0 and 1)
%\end{enumerate}
%\item New vision of each workflow
%\end{enumerate}
%\item Conclusion: world is better
%\end{enumerate}
%
%Current workflows
%\begin{enumerate}
%\item 
%\end{enumerate}
%
%How much of forecasting do we cover?
%\begin{enumerate}
%\item PBMs
%\item Near-term iterative
%\item SDMs
%\item Machine learning
%\end{enumerate}
%
%
%Miscellaneous notes/points without a home
%\begin{enumerate}
%\item Ecologists need to race the same data to make progress for trends and for forecasting (point to make at end of paper maybe? And what is the workflow for this?) ... though LPI is used a lot, perhaps it is  sign that ecology is ready to start racing the same data, but then we need `analysis-ready' data so we're not all slightly differently cleaning the data etc..
%\item Machine learning threatens utility of PBMs
%\item We need more uncertainty propagation for trends and forecasting (uncertainty is esp. ignored in PBMs)
%\item This workflow should lead to less model comparison (AIC, stepwise)
%\item This workflow works for machine learning!
%\item PBM: workflow should require estimating all parameters together; data simulation should reduce parameter number and highlight non-biol results
%\item Current trend workflow: Should be research question focused?
%\end{enumerate}
%
%Miscellaneous notes/points from thinking over 22-23 March weekend ...
%\begin{enumerate}
%\item Workflows emphasize there is no easy fix to better science or better stats
%\item Maybe do a retrodictive check example with LPI? Including showing how even the Freckleton work could do more? 
%\item What's the pathway from model comparison of many covariates to something else? Sometimes it seems like it's just prediction shoved into a mechanistic study, but if the goal is mechanism, there needs to be more work that either models these covariates together in a useful way (sort of approaching process-models!) or gets down to the fewer extremely relevant ones. 
%\item When do we need to open up the black box? Something about we need better training for what science is and our goals; machine learning is not often helping with advancing \emph{science}
%\item Scott Collins point that forecasting is not science (he claims a bunch of economists, weather folks etc. came to an ecological forecasting meeting and tried to explain that forecasting is an outcome of science, but it's not something you do science ON)
%\end{enumerate}

\clearpage
\bibliography{forecastflows}

\end{document}