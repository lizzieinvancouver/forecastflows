\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\bibliographystyle{besjournals}

\begin{document}

\title{Closing the gap between statistical and scientific workflows for improved forecasts in ecology } 
\date{\today}
\author{Victor Van der Meersch, J. Regetz, T. J. Davies$^*$ \& EM Wolkovich}
\maketitle
$^*$ Says he is happy to help and give friendly review, but not sure he will reach level of co-author. 

{\bf Deadline:} 1 May 2025 (was 1 April 2025)

\emph{For:} Scientific and Statistical Workflow theme issue for \emph{Phil Trans A} as an \emph{Opinion}
% They mention a tex template here: https://royalsocietypublishing.org/rsta/for-authors#question4
% Word length: I should check what they emailed but they say no more than 13 printed pages with 650 per page (whoa, we should be well below that! I am thinking 3-5K would be good)

\begin{abstract}
Increasing biodiversity loss and climate change have led to greater demands for useful ecological models and forecasts. Relevant datasets to meet these demands have also increased in size and complexity, including in their geographical, temporal and phylogenetic scales. While new research often suggests that accounting for these complexities variously increases, removes or otherwise alters major trends, I argue that the fundamental approach to model fitting in ecology makes it impossible to evaluate and compare models. These problems stem in part from continuing gaps between statistical workflows -- where the data processing and model development are often addressed separately from the ecological question and aim -- and scientific workflows, where all steps are integrated. Yet, as ecologists become increasingly computational, and new tools make it easier to share data, the opportunity to close this gap has never been greater. I outline how increased data simulation at multiple steps in the scientific workflow could revolutionize our understanding of ecological systems, yielding new insights. Combining these changes with more open model and data sharing -- and developing new efforts to race the same data -- could be transformative for ecological forecasting. 
\end{abstract}

{\noindent \bf Goal:} Increase awareness of how we can merge statistical and scientific workflows in ecology (especially forecasting) and what we would get out of it.
\vspace*{0.5cm}

\noindent \textbf{Introduction}
% Still need some work...

% Ecology has become super challenged to predict stuff for decision making
Nature is increasingly threatened by multiple drivers of change, with a largely dominant influence of human activities \citep{Diaz2019}. This ongoing biodiversity crisis is expected to increase in the next decades because of climate change, and will continue to alter ecosystem services and human well-being \citep{IPBES2019}. To support implementation of sustainable policies among the socioeconomic and environmental dimensions, it is critical to understand trends to date and be able to forecast future dynamics. 
% $\rightarrow$ Example of stuff to predict: population size and geographic distribution (terrestrial, freshwater, marine), extinction risk,...

% General way to do this so far... (bifurcated?)
Estimation of global biodiversity indicators and current trends depends on large-scale and long-term datasets---across terrestrial, freshwater, and marine ecosystems \citep[e.g.][]{Dornelas2018}. These data, gathered opportunistically and from multiple sources, are often unbalanced and have geographic, temporal and taxonomic biases. Addressing these biases requires the use of appropriate statistical inference.
Forecasting future changes---under different plausible scenarios---generally relies on either correlative models or process-based models \citep{IPBES2019}. The latter, which focus on a mechanistic representation of ecosystem functioning, are often promoted as the most realistic approach \citep{Urban2016, Pilowsky2022}. % ... lacks something

% Gap, problem: None of this is going well
The urgent need to answer policy-relevant questions has favored the proliferation of diverse methods developed by different researchers, lacking an overall coherence. Though there is no doubt nature is declining globally, significant uncertainty remains. There is no consensus on current species trends, with ongoing debates driven by widely varying reports that sometimes show conflicting trend directions \citep{Dornelas2014, Leung2020, Buschke2021, Johnson2024}. Future projections also diverge considerably, due to a high model uncertainty at the ecological level. Predictive modeling is increasingly relying on overly complex models (with a huge number of parameters), making it less adequate to generate new scientific insights \citep{Franklin2020}.

% Current workflows in ecology are not up to the task
% Here we introduce a universal workflow to adress this...
Current controversies and focus on methodological aspects stem from the lack of coherence between current workflows in ecology \citep{Loreau2022, Talis2023, Johnson2024}. Each new model development is added as a separate layer, disconnected from the original research aim, the data stream and the previous scientific insights. Workflows should fully integrate all the steps required to build a model from an ecological question, evaluate its limitations and degeneracies, before estimating its parameters and making projections. Here, we introduce an universal workflow that proposes to iteratively build upon all these steps, harmonizing both trend estimation and forecasting, with the aim of refocusing the debate on ecological questions and increasing the speed of scientific progress.

\vspace{0.5cm}
\noindent \textbf{Scientific method and workflows}
%  too much blah blah blah?

% General scientific method we all learn
Quantitative science rely on a model-based framework to confront hypothesis and data, making some approximations \citep{}. The general scientific method stresses out that the research question should guide both the design of the experiment and the corresponding model building. The experiment should then be conducted---according to this specific design. Finally, the resulting experimental data should be used to inform our model and differentiate between hypotheses---hopefully answering our initial question. However, this is often an idealized view and does not reflect the complexity of ecology \citep{}.

% Divergences from this are common 
Divergences from this scientific method are common. One explanation is the lack of rigor, leading to questionable practices, such as retrospectively crafting hypothesis to explain the results of a model rather than testing a clear pre-defined question (data-driven analysis, Fig. 1). But the reality of ecological research is also a major driver of the divergences from the ideal scientific method. Many important questions often cannot be addressed by conducting experiments and replications, and we must often rely on existing datasets to have a large-scale and long-term perspective. % I feel something is missing
However, this does not explain the persistent flaws and lack of overall coherence in ecological modeling. Trend estimation still often relies on a one-way 'inference',  % need a better word? to stress out that there is no feedback, eah paper applied a slightly different model 
and most of the time is spent fitting the model to empirical data (Fig. 1). For forecasting, researchers focus on making predictions with complex models, but the steps of model building and parameterization are not very transparent and not clearly delineated (Fig. 1). The different parts of the model are often calibrated separately rather than as a whole, and some parameter values are just fixed based on experiments and expert knowledge.

% There is thus room for improvment, with a workflow that addresses theses flaws while taking account the reality of working with ecological data.
% Moving along the data-model space
% \item stressing the need to think about model before study design
% \item advancing data simulation
[...]

% More details on the workflow:  walk through the different steps
[...]

\vspace{0.5cm}
\noindent \textbf{The workflow in pratice}
% How to address current issues (two case studies)

% Trends - outline current problem
[...]

% Trends - improvements!
[...]

% Forecasting - outline current problem
Forecasting is a broad field with a diverse range of methods. Here, we focus on process-based modeling, which is often considered the gold standard in ecology and beyond. Process-based models are built on explicit mathematical equations to describe (supposedly causal) relationships between environmental drivers and ecological responses. They also often incorporate empirical relationships, particularly when knowledge is incomplete or when some processes are intentionally omitted. Processes are often represented at different nested spatiotemporal scales, depending on the underlying assumptions. Model development is the central step, typically requiring several years, yet it often remains opaque from an external perspective. The step of designing the model---translating knowledge and hypotheses into mathematical equations and parameters---is often blurred with the step of model calibration, where parameter values are inferred. Models are often treated as an accumulation of multiple submodels, each governing one or several ecological processes. Rather than being fitted as a whole, submodels are calibrated separately against specific subsets of data, and some parameters are simply prescribed (i.e. fix to a value found in the literature).

% Forecasting - potential improvments (do I say too much ideas)
Applying the workflow to process-based models is a key for opening the black box. It would serve as a guide through the successive steps of model development. In particular, incorporating data simulation would introduce a crucial step between model building and data fitting, ensuring a clear delineation between the two and exposing strong degeneracies in the model design. This approach would force researchers to begin with a simpler version of the model, providing a clear pathway to support---or reject---the additional complexity and new parameters along the iterative development of the model.
Data fitting would no longer be just a hidden aspect of model building but a step as crucial as forecasting to gain new ecological insights. The workflow could also refocus attention on the research question, defining a clear and limited context in which the model should apply.
Process-based model would once again be a way to answer a research question---whereas today, model simulations have increasingly become a subject of study on their own. Finally, an universal workflow provides an opportunity to merge statistical and process-based approaches, integrating mechanistic knowledge and leveraging robust statistical approaches.

% Step back
%\item we need more data, and better question (relate this to both previous study cases)
%\item where can we best reduce uncertainties through new scientific insights?
%\item machine learning! If we change nothing, what's the point of not doing ML? ML $>$ process-based without question, and ML $>$ trends without mechanisms! Where theory is lacking, or where we are far from mechanistic understanding, you might as well do ML!
%\item (ML has a way to collect and interpret large datasets...)

\vspace{0.5cm}
\noindent \textbf{Wrap up: how to make it happen?}


%\item What are current workflows and where are they limiting us?
%\begin{enumerate}
%\item For trends ...
%\begin{enumerate}
%\item easy to find different trends through small model tweaks to analyses and/or different data
%\item For example, right now many different papers report different biodiversity trends (LPI example?)
%\item New workflow would make ecologists understand uncertainty in their model data/combo (and perhaps not see/publish results as so divergent?)
%\end{enumerate}
%\item For forecasting ... (somehow jump to our focus on process-based models PBMs here? Something like, forecasting is big and there are diverse methods! Near-term iterative, correlative niche models, but PBMs are often considered the gold standard ... mention machine learning?)
%\begin{enumerate}
%\item as many models as researchers working on process-based models 
%\item + accumulation of successive layers in the development of models = significant challenge to scientific transparency, reproducibility, interpretability\\
%models often draw inspiration from each other, which is good (way to do science), but not always explicit... (some issues: arbitrarily established parameter value in one model then transmitted to multiple models)
%\item focus of researchers: always integrate new mechanisms, new parameters, to increase "realism"...  they intuitively "feel" what kinds of adjustments is needed... but opaque from an external perspective ("black box" of model building and calibration)
%\item models rarely fitted as a whole, dozens of parameters without explicitly quantifying parameter uncertainty, and often neglect to propagate this uncertainty
%\item simulations of models themselves became a subject of study to disentangle all the processes modelled and understand model sensitivity 
%\end{enumerate}
%\end{enumerate}
%\item Better workflows to the rescue! 
%\begin{enumerate}
%\item General overview of new workflows
%\begin{enumerate}
%\item Step 0: Research Qs and hypotheses (with a mechanism) lets you ...
%\item Step 1: Build a model!
%\item Step 2: Simulate data (and priors or something like priors that forces you to put numbers on stuff)
%\item Step 3: Design experiments/data collection (maybe you go back to Step 1 here)
%\item Step 4: Simulate data from actual design/collection
%\item Step 5: Fit the model to empirical data
%\item Step 6: Retrodictive checks (feed back to 0 and 1)
%\end{enumerate}
%\item New vision of each workflow
%\end{enumerate}
%\item Conclusion: world is better
%\end{enumerate}
%
%Current workflows
%\begin{enumerate}
%\item 
%\end{enumerate}
%
%How much of forecasting do we cover?
%\begin{enumerate}
%\item PBMs
%\item Near-term iterative
%\item SDMs
%\item Machine learning
%\end{enumerate}
%
%
%Miscellaneous notes/points without a home
%\begin{enumerate}
%\item Ecologists need to race the same data to make progress for trends and for forecasting (point to make at end of paper maybe? And what is the workflow for this?) ... though LPI is used a lot, perhaps it is  sign that ecology is ready to start racing the same data, but then we need `analysis-ready' data so we're not all slightly differently cleaning the data etc..
%\item Machine learning threatens utility of PBMs
%\item We need more uncertainty propagation for trends and forecasting (uncertainty is esp. ignored in PBMs)
%\item This workflow should lead to less model comparison (AIC, stepwise)
%\item This workflow works for machine learning!
%\item PBM: workflow should require estimating all parameters together; data simulation should reduce parameter number and highlight non-biol results
%\item Current trend workflow: Should be research question focused?
%\end{enumerate}
%
%Miscellaneous notes/points from thinking over 22-23 March weekend ...
%\begin{enumerate}
%\item Workflows emphasize there is no easy fix to better science or better stats
%\item Maybe do a retrodictive check example with LPI? Including showing how even the Freckleton work could do more? 
%\item What's the pathway from model comparison of many covariates to something else? Sometimes it seems like it's just prediction shoved into a mechanistic study, but if the goal is mechanism, there needs to be more work that either models these covariates together in a useful way (sort of approaching process-models!) or gets down to the fewer extremely relevant ones. 
%\item When do we need to open up the black box? Something about we need better training for what science is and our goals; machine learning is not often helping with advancing \emph{science}
%\item Scott Collins point that forecasting is not science (he claims a bunch of economists, weather folks etc. came to an ecological forecasting meeting and tried to explain that forecasting is an outcome of science, but it's not something you do science ON)
%\end{enumerate}

\clearpage
\bibliography{forecastflows}

\end{document}