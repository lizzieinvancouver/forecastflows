\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\bibliographystyle{besjournals}

\begin{document}

\title{Closing the gap between statistical and scientific workflows for improved forecasts in ecology } 
\date{\today}
\author{Victor Van der Meersch, J. Regetz, T. J. Davies$^*$ \& EM Wolkovich}
\maketitle
$^*$ Says he is happy to help and give friendly review, but not sure he will reach level of co-author. 

{\bf Deadline:} 1 May 2025 (was 1 April 2025)

\emph{For:} Scientific and Statistical Workflow theme issue for \emph{Phil Trans A} as an \emph{Opinion}
% They mention a tex template here: https://royalsocietypublishing.org/rsta/for-authors#question4
% Word length: I should check what they emailed but they say no more than 13 printed pages with 650 per page (whoa, we should be well below that! I am thinking 3-5K would be good)

\begin{abstract}
Increasing biodiversity loss and climate change have led to greater demands for useful ecological models and forecasts. Relevant datasets to meet these demands have also increased in size and complexity, including in their geographical, temporal and phylogenetic scales. While new research often suggests that accounting for these complexities variously increases, removes or otherwise alters major trends, I argue that the fundamental approach to model fitting in ecology makes it impossible to evaluate and compare models. These problems stem in part from continuing gaps between statistical workflows -- where the data processing and model development are often addressed separately from the ecological question and aim -- and scientific workflows, where all steps are integrated. Yet, as ecologists become increasingly computational, and new tools make it easier to share data, the opportunity to close this gap has never been greater. I outline how increased data simulation at multiple steps in the scientific workflow could revolutionize our understanding of ecological systems, yielding new insights. Combining these changes with more open model and data sharing -- and developing new efforts to race the same data -- could be transformative for ecological forecasting. 
\end{abstract}

{\noindent \bf Goal:} Increase awareness of how we can merge statistical and scientific workflows in ecology (especially forecasting) and what we would get out of it.
\vspace*{0.5cm}

\noindent \textbf{Introduction}
% Still need some work...

% Ecology has become super challenged to predict stuff for decision making
Nature is increasingly threatened by multiple drivers of change, with a largely dominant influence of human activities \citep{Diaz2019}. This ongoing biodiversity crisis is expected to increase in the next decades because of climate change, and will continue to alter ecosystem services and human well-being \citep{IPBES2019}. To support implementation of sustainable policies among the socioeconomic and environmental dimensions, it is critical to understand trends to date and be able to forecast future dynamics. 
% $\rightarrow$ Example of stuff to predict: population size and geographic distribution (terrestrial, freshwater, marine), extinction risk,...

% General way to do this so far... (bifurcated?)
Estimation of global biodiversity indicators and current trends depends on large-scale and long-term datasets---across terrestrial, freshwater, and marine ecosystems \citep[e.g.][]{Dornelas2018}. These data, gathered opportunistically and from multiple sources, are often unbalanced and have geographic, temporal and taxonomic biases. Addressing these biases requires the use of appropriate statistical inference.
Forecasting future changes---under different plausible scenarios---generally relies on either correlative models or process-based models \citep{IPBES2019}. The latter, which focus on a mechanistic representation of ecosystem functioning, are often promoted as the most realistic approach \citep{Urban2016, Pilowsky2022}. % ... lacks something

% Gap, problem: None of this is going well
The urgent need to answer policy-relevant questions has favored the proliferation of diverse methods developed by different researchers, lacking an overall coherence. Though there is no doubt nature is declining globally, significant uncertainty remains. There is no consensus on current species trends, with ongoing debates driven by widely varying reports that sometimes show conflicting trend directions \citep{Dornelas2014, Leung2020, Buschke2021, Johnson2024}. Future projections also diverge considerably, due to a high model uncertainty at the ecological level. Predictive modeling is increasingly relying on overly complex models (with a huge number of parameters), making it less adequate to generate new scientific insights \citep{Franklin2020}.

% Current workflows in ecology are not up to the task
% Here we introduce a universal workflow to adress this...
Current controversies and focus on methodological aspects stem from the lack of coherence between current workflows in ecology \citep{Loreau2022, Talis2023, Johnson2024}. Each new model development is added as a separate layer, disconnected from the original research aim, the data stream and the previous scientific insights. Workflows should fully integrate all the steps required to build a model from an ecological question, evaluate its limitations and degeneracies, before estimating its parameters and making projections. Here, we introduce an universal workflow that proposes to iteratively build upon all these steps, harmonizing both trend estimation and forecasting, with the aim of refocusing the debate on ecological questions and increasing the speed of scientific progress.

\vspace{0.5cm}
\noindent \textbf{Scientific method and workflows}
%  too much blah blah blah?

% General scientific method we all learn
Quantitative science rely on a model-based framework to confront hypothesis and data, making some approximations \citep{}. The general scientific method stresses out that the research question should guide both the design of the experiment and the corresponding model building. The experiment should then be conducted---according to this specific design. Finally, the resulting experimental data should be used to inform our model and differentiate between hypotheses---hopefully answering our initial question. However, this is often an idealized view and does not reflect the complexity of ecology \citep{}.

% Divergences from this are common 
Divergences from this scientific method are common. One explanation is the lack of rigor, leading to questionable practices, such as retrospectively crafting hypothesis to explain the results of a model rather than testing a clear pre-defined question (data-driven analysis, Fig. 1). But the reality of ecological research is also a major driver of the divergences from the ideal scientific method. Many important questions often cannot be addressed by conducting experiments and replications, and we must often rely on existing datasets to have a large-scale and long-term perspective. % I feel something is missing
However, this does not explain the persistent flaws and lack of overall coherence in ecological modeling. Trend estimation still often relies on a one-way 'inference',  % need a better word? to stress out that there is no feedback, eah paper applied a slightly different model 
and most of the time is spent fitting the model to empirical data (Fig. 1). For forecasting, researchers focus on making predictions with complex models, but the steps of model building and parameterization are not very transparent and not clearly delineated (Fig. 1). The different parts of the model are often calibrated separately rather than as a whole, and some parameter values are just fixed based on experiments and expert knowledge.

% There is thus room for improvment, with a workflow that addresses theses flaws while taking account the reality of working with ecological data.
% Moving along the data-model space
% \item stressing the need to think about model before study design
% \item advancing data simulation
[...]

% More details on the workflow:  walk through the different steps
[...]

\vspace{0.5cm}
\noindent \textbf{The workflow in practice}
% How to address current issues (two case studies)

% Trends - outline current problem
%  Evidence of declining populations of vertebrate species in the 20th century gave rise a new subfield within ecology---conservation biology---now an entire discipline of its own that drives much of the policy-relevant science in ecology. 
Evidence of declining populations of vertebrate species in the latter half of the 20th century, alongside increasing concerns of how pollution affects ecosystems and their services gave rise to a growing public concern about protecting and maintaining the environment, and made ecology as a field more relevant \cite{soule1999conserving,soule1991conservation}. The idea that important taxa were declining was clear from data from certain species and their populations, such as elephants and rhinos \citep{soule1979benign,leader1990illegal}. Such trends drove number of new subfields within ecology---some of which are now complete disciplines within themselves, such as conservation biology \citep{soule1985conservation}---focused on these problems, and potential ecological solutions to them CITES. Yet as the magnitude and number of threats to these and other species have increased, with rates of habitat loss, overhavesting, pollution only increase, and anthropogenic climate change now clearly driving species loss (CITE bramble cay meomys), so has the data and its complexity. 

% \cite{loh2005living} is first LPI paper as best I know
% \cite{collen2009monitoring,ledger2023past,leung2022reply,puurtinen2022living,toszogyova2024mathematical} to review!
As ecologists have now amassed data on populations and species over the globe, they have also engaged in an increasing number of debates on regional and global trends, with arguments over the magnitude and even direction (CITES). The Living Planet Index (LPI), which aims to include long-term data on populations of species across the globe is emblematic of these debates (which also include whether insects are declining and ... CITES). With updated data released semi-annually (??) alongside new estimates of decline, a growing number of high-profile papers have challenged how strong the evidence is for population decline (CITES), with each paper taking a slightly different analytical approach. For example, \citet{Leung2020} published a mixture model that suggested most populations were not significantly declining, followed by \citet{Buschke2021} who focused on adding random population variation. Recently, \citep{Johnson2024} suggested a basic analysis of the dataset should always include three sources of autocorrelation, finding trends that encompassed most previous results. These apparently conflicting results have made it hard for policy-makers to do advance initiatives aimed at slowing declines, and has led to a debates within ecology about whether such analyses undermine public confidence in science  \citep{gonzalez2016estimating}. While shifting estimates are part of the process of science---refining our approaches and thus estimates over time---we argue much of the work underlying these debates stems from a poor workflow. In the current workflow for estimating trends over time a new model with a new estimate often leads to a paper (see Fig.) because ecologists spend far too little time interrogating their model with simulated data, or their model performance fit to empirical data. 
% While this debate captures part of the process of science---refining our approaches and thus estimates over time---it also makes it hard for policy-makers to do (?? something) and has led to a debates within ecology about whether such analyses undermine public confidence in science  (CITES Biodiversity debate). 

We argue than an improved workflow that required retrodictive checks and data simulation would lead to larger model advances and highlight consistency in estimates across models, which could better aid policy. This is because the workflow would make uncertainty in each trend estimate from each unique model more obvious. This would make what now appear as major discrepancies  more obviously shifts in point estimates that are generally all in the same uncertainty space---and because it would challenge modelers to show major predictive advances, which is not currently part of the process. Explanatory power in most models of observational data is usually very low (CITES) and thus tests of models' predictions rarely expected. But the workflow highlights that predictions from the model---what we call retrodictive checks (or whatever we call them)--are part of the process of science, and critical to testing for what may be missing in a model. We expect retrodictive checks on most published trend analyses would highlight major missing components in these models (expand here?? ADD example?), and drive changes both in the models themselves and in the simulated data to check the models. 
% Mention how low R2 are in ecology?

% Note to self for Lizzie: START HERE! And no reading/reviewing papers until more text is written...
Simulating data to verify and test models in trend analyses is currently rarely reported, which we believe makes it easier for ecologists to fit poor models 
% Mention elephants rebounding...
Indeed, entire papers are devoted to what we consider this one step (data simulation) ....
% I need to review these papers! \cite{hourdin2023toward,mcrae2025utility} ... also \cite{Buschke2021} uses some simulations




% Trends - improvements!
[...]

% Forecasting - outline current problem
Forecasting is a broad field with a diverse range of methods. Here, we focus on process-based modeling, which is often considered the gold standard in ecology and beyond. Process-based models are built on explicit mathematical equations to describe (supposedly causal) relationships between environmental drivers and ecological responses. They also often incorporate empirical relationships, particularly when knowledge is incomplete or when some processes are intentionally omitted. Processes are often represented at different nested spatiotemporal scales, depending on the underlying assumptions. Model development is the central step, typically requiring several years, yet it often remains opaque from an external perspective. The step of designing the model---translating knowledge and hypotheses into mathematical equations and parameters---is often blurred with the step of model calibration, where parameter values are inferred. Models are often treated as an accumulation of multiple submodels, each governing one or several ecological processes. Rather than being fitted as a whole, submodels are calibrated separately against specific subsets of data, and some parameters are simply prescribed (i.e. fix to a value found in the literature).

% Forecasting - potential improvments (do I say too much ideas)
Applying the workflow to process-based models is a key for opening the black box. It would serve as a guide through the successive steps of model development. In particular, incorporating data simulation would introduce a crucial step between model building and data fitting, ensuring a clear delineation between the two and exposing strong degeneracies in the model design. This approach would force researchers to begin with a simpler version of the model, providing a clear pathway to support---or reject---the additional complexity and new parameters along the iterative development of the model.
Data fitting would no longer be just a hidden aspect of model building but a step as crucial as forecasting to gain new ecological insights. The workflow could also refocus attention on the research question, defining a clear and limited context in which the model should apply.
Process-based model would once again be a way to answer a research question---whereas today, model simulations have increasingly become a subject of study on their own. Finally, an universal workflow provides an opportunity to merge statistical and process-based approaches, integrating mechanistic knowledge and leveraging robust statistical approaches.

% Step back
%\item we need more data, and better question (relate this to both previous study cases)
%\item where can we best reduce uncertainties through new scientific insights?
%\item machine learning! If we change nothing, what's the point of not doing ML? ML $>$ process-based without question, and ML $>$ trends without mechanisms! Where theory is lacking, or where we are far from mechanistic understanding, you might as well do ML!
%\item (ML has a way to collect and interpret large datasets...)
Across the different fields of ecology---for both parameter estimation and forecasting---a systematic application of a coherent workflow holds the promise to highlight the opportunities to best reduce uncertainties through new scientific insights, toward the most critical steps. This will help refocus the debate on designing new hypothesis, formulating new questions---and guiding efforts to collect new data. 
In a world where machine learning is rapidly advancing, there is no point of sticking to traditional methods if no changes are made. Machine learning may surpass process-based models if the latter lack a robust estimation of their parameters and fall in a complexity trap, at the cost of their interpretability. Similarly, trend analysis, when the focus is on methodological controversies (due to the lack of an iterative workflow) rather than on a robust mechanistic foundation, offers no clear advantage over machine learning.
% If we do not take care to apply this workflow, machine learning might be the more pragmatic approach...

% Misc possibly useful citations -- the GCM people seem to talk a lot about their issues lately on tuning etc. Maybe worth citing?
% \cite{balaji2022general}mAre general circulation models obsolete? -- covers a lot and good citations within I think
% \cite{simpson2025confronting} this is the 'Confronting Earth System Model trends with observations' which I have not fully read but Table 1 is interesting to me and something maybe the PBM and NTIF (near-term iterative) people should be doing if they are not doing?
% \cite{rounce2020quantifying} 'Quantifying parameter uncertainty in a large-scale glacier evolution model using Bayesian inference: application to High Mountain Asia' talks about identifablity problems and uses Bayesian, again I have not fully read, but may be interesting example?

\vspace{0.5cm}
\noindent \textbf{Wrap up: how to make it happen?}

The misuse of models and misinterpretation of their outcomes (including by the authors) have multiple origins. One cause, almost tautological, is the pressure to publish academic work, which can lower research standards. This challenge extends beyond the scope of this paper, yet we believe that applying an intelligible workflow can promote better research ethics.
This a growing concern, leading to increase in reproducibility and data sharing practices.
However, model development in ecology is still rarely transparent, which limits model understandability and prevents peers from properly identifying potential issues. A significant portion of scientific debate thus becomes lost in methodological considerations rather than advancing our ecological understanding.



%\item What are current workflows and where are they limiting us?
%\begin{enumerate}
%\item For trends ...
%\begin{enumerate}
%\item easy to find different trends through small model tweaks to analyses and/or different data
%\item For example, right now many different papers report different biodiversity trends (LPI example?)
%\item New workflow would make ecologists understand uncertainty in their model data/combo (and perhaps not see/publish results as so divergent?)
%\end{enumerate}
%\item For forecasting ... (somehow jump to our focus on process-based models PBMs here? Something like, forecasting is big and there are diverse methods! Near-term iterative, correlative niche models, but PBMs are often considered the gold standard ... mention machine learning?)
%\begin{enumerate}
%\item as many models as researchers working on process-based models 
%\item + accumulation of successive layers in the development of models = significant challenge to scientific transparency, reproducibility, interpretability\\
%models often draw inspiration from each other, which is good (way to do science), but not always explicit... (some issues: arbitrarily established parameter value in one model then transmitted to multiple models)
%\item focus of researchers: always integrate new mechanisms, new parameters, to increase "realism"...  they intuitively "feel" what kinds of adjustments is needed... but opaque from an external perspective ("black box" of model building and calibration)
%\item models rarely fitted as a whole, dozens of parameters without explicitly quantifying parameter uncertainty, and often neglect to propagate this uncertainty
%\item simulations of models themselves became a subject of study to disentangle all the processes modelled and understand model sensitivity 
%\end{enumerate}
%\end{enumerate}
%\item Better workflows to the rescue! 
%\begin{enumerate}
%\item General overview of new workflows
%\begin{enumerate}
%\item Step 0: Research Qs and hypotheses (with a mechanism) lets you ...
%\item Step 1: Build a model!
%\item Step 2: Simulate data (and priors or something like priors that forces you to put numbers on stuff)
%\item Step 3: Design experiments/data collection (maybe you go back to Step 1 here)
%\item Step 4: Simulate data from actual design/collection
%\item Step 5: Fit the model to empirical data
%\item Step 6: Retrodictive checks (feed back to 0 and 1)
%\end{enumerate}
%\item New vision of each workflow
%\end{enumerate}
%\item Conclusion: world is better
%\end{enumerate}
%
%Current workflows
%\begin{enumerate}
%\item 
%\end{enumerate}
%
%How much of forecasting do we cover?
%\begin{enumerate}
%\item PBMs
%\item Near-term iterative
%\item SDMs
%\item Machine learning
%\end{enumerate}
%
%
%Miscellaneous notes/points without a home
%\begin{enumerate}
%\item Ecologists need to race the same data to make progress for trends and for forecasting (point to make at end of paper maybe? And what is the workflow for this?) ... though LPI is used a lot, perhaps it is  sign that ecology is ready to start racing the same data, but then we need `analysis-ready' data so we're not all slightly differently cleaning the data etc..
%\item Machine learning threatens utility of PBMs
%\item We need more uncertainty propagation for trends and forecasting (uncertainty is esp. ignored in PBMs)
%\item This workflow should lead to less model comparison (AIC, stepwise)
%\item This workflow works for machine learning!
%\item PBM: workflow should require estimating all parameters together; data simulation should reduce parameter number and highlight non-biol results
%\item Current trend workflow: Should be research question focused?
%\end{enumerate}
%
%Miscellaneous notes/points from thinking over 22-23 March weekend ...
%\begin{enumerate}
%\item Workflows emphasize there is no easy fix to better science or better stats
%\item Maybe do a retrodictive check example with LPI? Including showing how even the Freckleton work could do more? 
%\item What's the pathway from model comparison of many covariates to something else? Sometimes it seems like it's just prediction shoved into a mechanistic study, but if the goal is mechanism, there needs to be more work that either models these covariates together in a useful way (sort of approaching process-models!) or gets down to the fewer extremely relevant ones. 
%\item When do we need to open up the black box? Something about we need better training for what science is and our goals; machine learning is not often helping with advancing \emph{science}
%\item Scott Collins point that forecasting is not science (he claims a bunch of economists, weather folks etc. came to an ecological forecasting meeting and tried to explain that forecasting is an outcome of science, but it's not something you do science ON)
%\end{enumerate}

\clearpage
\bibliography{forecastflows}

\end{document}